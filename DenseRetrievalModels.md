# Dense Retrieval Models

## 1. Why Do We Need Dense Retrieval Models?
Traditional search methods like **TF-IDF** and **BM25** rely on **keyword matching**. However, they have limitations:
- They **don't understand meaning**, only exact words.
- They **struggle with synonyms** (e.g., "car" vs. "automobile").
- They **fail in complex queries** where context matters.

Dense retrieval models go beyond keywords by using **embeddings** to capture the meaning of words and sentences.

## 2. How Do Dense Retrieval Models Work?
Dense retrieval models convert text into **dense vectors (embeddings)** using neural networks. These embeddings are then matched using **vector similarity (e.g., cosine similarity)** instead of exact keyword matches.

## 3. What is DPR (Dense Passage Retrieval)?
Dense Passage Retrieval (DPR) is a **neural network-based retrieval model** designed for **open-domain question answering**. It retrieves relevant passages from a large corpus **based on meaning, not just keywords**.

### Is DPR Widely Used?
Yes, DPR is widely used in:
- **Question Answering Systems (like OpenAI and Google Search)**
- **Chatbots and Conversational AI**
- **Document Search Engines**
- **Legal, Medical, and Financial Text Retrieval**

## 4. How DPR Works
DPR uses a **dual-encoder architecture** to encode both queries and passages into vector representations, making retrieval efficient.

### What Are Embeddings?
Embeddings are **numerical representations** of words, sentences, or documents. Similar texts have **closer embeddings**, enabling effective retrieval beyond keyword matching.

### What is Dual-Encoder Architecture?
DPR consists of two encoders:
- **Query Encoder**: Converts user queries into vectors.
- **Passage Encoder**: Converts candidate passages into vectors.

### Why Use Dual Encoders?
- Instead of computing similarity at search time, DPR precomputes passage embeddings, making retrieval faster.
- Allows **efficient similarity search** using methods like **FAISS (Facebook AI Similarity Search)**.

## 5. Training Process of DPR
DPR is trained using **contrastive learning**, which involves **positive and negative examples**.

### What Are Positive and Negative Examples?
- **Positive Example**: A passage that correctly answers a given query.
- **Negative Example**: A passage that looks similar but does not contain the answer.

### Example:
Query: *"Who invented the telephone?"*
- **Positive Passage**: *"Alexander Graham Bell invented the telephone in 1876."*
- **Negative Passage**: *"The telephone is a communication device used worldwide."*

The model **learns to pull positive passages closer** and **push negative passages further apart** in embedding space.

## 6. How DPR Learns Relevance
DPR optimizes **embedding similarity** using:
- **Triplet Loss** or **Contrastive Loss** to separate relevant and irrelevant passages.
- **Hard Negative Mining**: Finding tricky negative examples to improve learning.
- **Fine-tuning on domain-specific data** to improve retrieval accuracy.

## Conclusion
DPR and dense retrieval models **go beyond keywords** to understand the **semantic meaning** of text. They are crucial for modern **question answering, document retrieval, and conversational AI**, outperforming traditional keyword-based search methods like BM25.

---

Let me know if you need any modifications! ðŸš€

